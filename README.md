# Laboratorio-2-se침ales-
>  Problema del coctel 
---

Septiembre 2024

## Tabla de contenidos
* [쯈u칠 se va a realizar?](#introduccion)
* [Problema del coctel](#problema)
* [Configuracion del sistema](#configuracion)
* [Librerias](#librerias)
* [Cargar audios de voces y rudios](#carga)
* [Graficas de onda del audio](#onda)
* [Espectro de frecuencia](#espectro)
* [Mezcla archivos de audio](#mezcla)
* [Uso de ICA, separacion y SNR relacion se침al-ruido](#ica)
* [Calculo SNR voces separadas](#snr)
* [Menu](#menu)
* [Analisis](#analisis)
* [Contacto](#contacto)
---
<a name="introduccion"></a> 
## 쯈u칠 se va a realizar?

En un evento tipo coctel, se instalaron varios micr칩fonos para escuchar lo que las personas estaban hablando; una vez termin칩 la fiesta, se solicit칩 a los 
ingenieros que entregaran el audio de la voz de uno de los participantes. Los ingenieros analizaron las se침ales grabadas por los micr칩fonos eran mezclas 
de se침ales que proven칤an de diferentes fuentes (personas) para todos los casos y se encontraron con el problema de aislar la voz de inter칠s. 

El problema de la "fiesta de c칩ctel" se refiere a la capacidad de un sistema para concentrarse en una sola fuente sonora mientras filtra las dem치s en un entorno con m칰ltiples emisores de sonido. Este problema es com칰n en sistemas de audici칩n tanto humanos como artificiales, y su resoluci칩n es esencial en 
aplicaciones como la mejora de la voz, el reconocimiento de habla y la cancelaci칩n de ruido. 

Para este laboratorio, los estudiantes recrear치n el problema de la fiesta de coctel, donde existen 洧녵 fuentes sonoras capturadas por un arreglo de 洧녵 
micr칩fonos de acuerdo con la siguiente metodolog칤a. 
1. Configuraci칩n del sistema: 
- Conecta los tres micr칩fonos al sistema de adquisici칩n de datos y distribuy칠ndolos estrat칠gicamente en la sala. Los micr칩fonos deben estar ubicados de manera que cada uno capture diferentes combinaciones de las se침ales provenientes de las tres fuentes.
- Ubicar tres personas en posiciones fijas dentro de la sala de laboratorio.
- Deben estar localizados a distancias diferentes y orientados en distintas direcciones para simular un entorno de "fiesta de c칩ctel". 
2. Captura de la se침al:
- Generar la se침al mediante la voz de los tres sujetos de prueba; cada uno debe decir una frase diferente durante el tiempo de captura de la se침al. Las se침ales de los micr칩fonos deben ser registradas por el sistema de adquisici칩n y guardas para ser analizadas.
- Grabar el ruido de la sala con los 3 micr칩fonos y calcular el SNR de cada se침al. Repetir la medici칩n en caso de que el SNR de alguna de las se침ales 
no sea suficiente. Argumentar las razones. 
3. Procesamiento de se침ales: 
- Realizar un an치lisis temporal y espectral de las se침ales capturadas por cada micr칩fono, identificando las caracter칤sticas principales de cada fuente
sonora. Para realizar el an치lisis espectral, se recomienda utilizar la transformada de Fourier discreta (DFT) o la transformada r치pida de Fourier (FFT),
describiendo la informaci칩n que se puede obtener con cada una de ellas.
- Investigar los m칠todos de separaci칩n de fuentes, como el An치lisis de Componentes Independientes (ICA), el Beamforming, entre otros, para aislar 
la se침al de inter칠s a partir de las se침ales capturadas por los micr칩fonos.  
4. Evaluar los resultados comparando la se침al aislada con la se침al original utilizado m칠tricas de calidad como la relaci칩n se침al/ruido para cuantificar el desempe침o de la separaci칩n. 

<a name="problema"></a> 
## Problema del coctel

El efecto de fiesta de c칩ctel es un fen칩meno que consiste en enfocar la atenci칩n auditiva en un est칤mulo ac칰stico en particular, mientras se trata de filtrar y eliminar el resto de est칤mulos que pueden actuar como distractores.El nombre de este fen칩meno es bastante representativo del efecto, dado que, si lo pensamos, en una fiesta, cuando estamos hablando con alg칰n invitado, tratamos de filtrar lo que nos est치 diciendo e ignorando la m칰sica y otras conversaciones que puedan estar transcurriendo de forma simult치nea, conformando el fondo.

Gracias a este fen칩meno, somos capaces de diferenciar entre la voz de la persona con quien estamos manteniendo la conversaci칩n de la del resto de personas que puedan estar conformando el fondo ac칰stico del ambiente en el que nos estamos encontrando.Este mismo fen칩meno tambi칠n es el que permite que, sin estar del todo concentrado en otras conversaciones, seamos capaces de captar la atenci칩n cuando se menciona una palabra que nos resulta importante, como puede ser el que nos llamen por nuestro nombre.

<a name="configuracion"></a> 
## Configuracion del sistema

En este caso como fue mencionado en la introduccion se debia realizar con 3 personas, en un inicio el laboratorio si se hizo de esa manera pero los audios tomados fueron de un tiempo muy bajo (1 segundo), por esta razon al no obtener un buen muestro decidio realizarse una nueva toma con 2 personas de  la siguiente manera:

Las personas se organizaron una en frente de la otra con una distancia de 27,9 cm equivalemte a lo que mide una hoja carta de largo desde los pies de la persona 1 al celular 1, luego otra hoja desde el celular 1 al celular 2 y una ultima hoja desde el celular 2 a la persona 2. 

![Agregar](captura.jpg)

La grabacion se realizo en este caso por medio una aplicacion llamada "Grabadora de Voz" la cual la podemos encontrar en app store, esto para asegurar que la frecuencia de muestreo que utilizaramos sea la misma.

![Agregar](grabadora.jpg)

Para la grabacion la frecuencia de muestreo fue de 16 kHz en cada celular, se grabaron 4 segundos del ruido del ambiente y 4 segundos a las personas hablando.

![Agregar](frecuencia.jpg)

1. Audios persona 1  (Mariana):


https://github.com/user-attachments/assets/e087082f-2b14-4eb6-b42e-ab8e2317dc59

Dialogo: Mi nombre es Mariana Higuera, tengo 18 a침os y mi mam치 se llama Lilian Caicedo.

https://github.com/user-attachments/assets/cd83c1c9-4014-4715-8696-32f6dae8d133

2. Audio persona 2 (Mam치):

https://github.com/user-attachments/assets/880810c3-7b88-43a7-87cd-1a991a7554f7

Dialogo: Mi mam치 se llama Lilia Vasquez, yo tengo 51 a침os y mi nombre es Lilian Caicedo.

https://github.com/user-attachments/assets/ca4394b2-ae84-47e0-8e82-1a0b656d8360

<a name="librerias"></a> 
## Librerias
```c
import matplotlib.pyplot as plt
import numpy as np
import librosa
import sounddevice as sd
from sklearn.decomposition import FastICA
```
1. Librosa: Es una biblioteca especializada en el procesamiento de se침ales de audio. Es ampliamente utilizada para tareas como la extracci칩n de caracter칤sticas de audio, la generaci칩n de espectrogramas, y la manipulaci칩n de archivos de audio.

2. SoundDevice: Es una biblioteca que permite la grabaci칩n y reproducci칩n de audio directamente desde Python usando dispositivos de audio.

3. FastICA (de sklearn.decomposition): Es una implementaci칩n del algoritmo de An치lisis de Componentes Independientes (ICA) en la biblioteca scikit-learn. ICA es una t칠cnica de separaci칩n de fuentes ciega (BSS) que intenta descomponer una se침al multivariable en componentes estad칤sticamente independientes.

- Compatibilidad y Estandarizaci칩n: .wav es un formato ampliamente compatible con diversas herramientas y bibliotecas de procesamiento de audio, incluidas librosa y sounddevice, que se utilizan en este c칩digo. Esto asegura que no haya problemas de compatibilidad al cargar o reproducir archivos de audio.

<a name="carga"></a> 
## Cargar audios de voces y ruidos

Este c칩digo est치 dise침ado para trabajar con archivos de audio, espec칤ficamente para cargar y reproducir grabaciones de voces y ruidos almacenados en archivos de formato `.wav`. El c칩digo comienza definiendo dos listas, `audio_voces` y `audio_ruido`, que contienen los nombres de los archivos de audio correspondientes a las voces y ruidos que se desean manipular.

A continuaci칩n, se define una funci칩n llamada `loadAudio`, que utiliza la biblioteca `librosa` para cargar un archivo de audio. Esta funci칩n toma como par치metro el nombre o la ruta de un archivo de audio y devuelve dos cosas: un array NumPy que contiene los datos de la se침al de audio y la tasa de muestreo con la que fue grabado el archivo. La tasa de muestreo es la cantidad de muestras por segundo que se toman del audio, y se mantiene intacta (sin cambios) al cargar el archivo.

La otra funci칩n, `playAudio`, se encarga de reproducir el audio que ha sido cargado. Esta funci칩n utiliza la biblioteca `sounddevice` para enviar los datos de audio al dispositivo de salida, como los altavoces o auriculares. La funci칩n recibe los datos de audio y la tasa de muestreo, los cuales son necesarios para que la reproducci칩n sea precisa. Adem치s, se incluye un comando para esperar hasta que la reproducci칩n del audio haya terminado antes de proceder con cualquier otra operaci칩n en el c칩digo.

```c
# Archivos de voces y ruidos ya cargados
audio_voces = ["voz_mama.wav", "voz_mari.wav"]
audio_ruido = ["Ruidomama.wav", "ruidomari.wav"]

# Funci칩n para cargar un archivo de audio
def loadAudio(archivo):
    data, samplerate = librosa.load(archivo, sr=None)  # Carga el archivo de audio sin cambio de tasa de muestreo
    return data, samplerate

# Funci칩n para reproducir un audio
def playAudio(data, sr):
    sd.play(data, sr)  # Reproduce el audio
    sd.wait()  # Espera a que termine la reproducci칩n
```

<a name="onda"></a> 
## Graficas de onda del audio

La funci칩n `graficarSonido` est치 dise침ada para crear una visualizaci칩n de la forma de onda de un archivo de audio. En primer lugar, la funci칩n calcula el tiempo en segundos para cada muestra de audio. Esto se realiza generando un array de 칤ndices que representan cada muestra en el archivo y dividiendo estos 칤ndices por la tasa de muestreo, `sr`. El resultado es un array que indica el tiempo correspondiente a cada muestra, permitiendo que el eje x del gr치fico represente el tiempo en segundos.

Luego, la funci칩n crea una figura de tama침o 12x6 pulgadas para la visualizaci칩n. Utiliza `matplotlib` para graficar los datos de audio, donde el eje x muestra el tiempo y el eje y muestra la amplitud de la se침al. La l칤nea del gr치fico se dibuja en color azul con un grosor de 1.5 puntos. Para enfocar la visualizaci칩n, el eje x se limita a los primeros 2 segundos del audio, lo que es 칰til si se desea examinar solo una parte espec칤fica de la se침al.

```c
# Funci칩n para graficar la forma de onda del audio
def graficarSonido(data, sr, title=""):
    t = np.arange(len(data)) / sr  # Calcula el tiempo en segundos
    plt.figure(figsize=(12, 6))  # Tama침o de la figura
    plt.plot(t, data, color='royalblue', lw=1.5)  # Grafica la forma de onda en color azul
    plt.xlim(0, 2)  # Limita el eje x a los primeros 2 segundos
    plt.xlabel('Tiempo (s)')
    plt.ylabel('Amplitud')
    plt.title(title)
    plt.grid(True, linestyle='--', alpha=0.7)  # A침ade una cuadr칤cula punteada
    plt.show()
```

![Agregar](ondamama.png)

![Agregar](ondamari.png)

<a name="espectro"></a> 
## Espectro de frecuencia

La funci칩n `analisisEspectral` est치 dise침ada para analizar y visualizar el espectro de frecuencias de una se침al de audio. Primero, la funci칩n calcula el n칰mero total de muestras del audio mediante `n = len(data)`. Tambi칠n determina el intervalo de muestreo `T`, que es el inverso de la tasa de muestreo `sr`, indicando el tiempo entre cada muestra.

La funci칩n luego aplica la Transformada R치pida de Fourier (FFT) a los datos de audio con `np.fft.fft(data)`. La FFT convierte la se침al del dominio del tiempo al dominio de la frecuencia, permitiendo analizar las frecuencias presentes en el audio. Para obtener las frecuencias correspondientes a la FFT, se usa `np.fft.fftfreq(n, T)`, y se toma solo la primera mitad de las frecuencias (`[:n // 2]`), ya que la FFT produce una imagen sim칠trica en el dominio de la frecuencia.

- Transformada r치pida de Fourier FFT: Es un algoritmo utilizado para calcular la transformada discreta de Fourier (DFT) y su inversa de manera m치s eficiente. La DFT es una transformada utilizada en el procesamiento de se침ales y de im치genes, entre muchas otras 치reas, para transformar una se침al discreta en su representaci칩n en el dominio de la frecuencia. La FFT acelera el proceso de c치lculo de la DFT, lo que permite su uso en aplicaciones en tiempo real y para grandes conjuntos de datos.

```c
# Funci칩n para analizar el espectro de frecuencias del audio
def analisisEspectral(data, sr, title="Espectro de frecuencias del audio"):
    n = len(data)  # N칰mero de muestras
    T = 1 / sr  # Intervalo de muestreo
    yf = np.fft.fft(data)  # Transformada de Fourier del audio
    xf = np.fft.fftfreq(n, T)[:n // 2]  # Frecuencias correspondientes a la FFT
    
    plt.figure(figsize=(12, 6))  # Tama침o de la figura
    plt.plot(xf, 2.0 / n * np.abs(yf[:n // 2]), color='darkorange', lw=1.5)  # Grafica el espectro en color naranja
    plt.xlim(0, 2000)  # Limita el eje x a 2000 Hz
    plt.xlabel('Frecuencia (Hz)')
    plt.ylabel('Amplitud')
    plt.title(title)
    plt.grid(True, linestyle='--', alpha=0.7)  # A침ade una cuadr칤cula punteada
    plt.show()
```

![Agregar](espectromama.png)

![Agregar](espectromari.png)

<a name="mezcla"></a> 
## Mezcla archivos de audio

La funci칩n `mezclarVoces` est치 dise침ada para combinar dos archivos de audio en una sola se침al, permitiendo mezclarlos para obtener una mezcla de las dos grabaciones. 

Primero, la funci칩n carga los datos de ambos archivos de audio utilizando la funci칩n `loadAudio`. Esto se realiza llamando a `loadAudio(audio1)` y `loadAudio(audio2)`, lo que devuelve dos conjuntos de datos de audio y la tasa de muestreo (`sr`). La tasa de muestreo es la misma para ambos archivos, ya que `loadAudio` se encargar치 de mantener la tasa original.

Luego, la funci칩n asegura que ambas se침ales de audio tengan la misma longitud para evitar problemas en el procesamiento posterior. Calcula la longitud m칤nima entre las dos se침ales usando `min(len(data1), len(data2))` y ajusta ambas se침ales a esta longitud. Esto garantiza que tanto `data1` como `data2` tengan el mismo n칰mero de muestras al recortar el exceso de datos en la se침al m치s larga.

Las se침ales ajustadas se almacenan en una lista llamada `signals`. Luego, se utiliza `np.vstack(signals)` para apilar las dos se침ales en una matriz, donde cada fila representa una de las se침ales. Finalmente, la matriz se transpone con `mixed_signals.T` para que cada se침al ocupe una columna en lugar de una fila.

El resultado de la funci칩n es una matriz en la que cada columna corresponde a una de las se침ales originales mezcladas, y se devuelve junto con la tasa de muestreo `sr`. Esta estructura facilita la manipulaci칩n y an치lisis de las se침ales mezcladas, permitiendo trabajar con ambas se침ales simult치neamente en el mismo formato.

```c
# Funci칩n para mezclar dos archivos de audio
def mezclarVoces(audio1, audio2):
    signals = []
    data1, sr = loadAudio(audio1)  # Carga el primer archivo
    data2, sr = loadAudio(audio2)  # Carga el segundo archivo
    
    # Ajusta la longitud para que ambas se침ales tengan el mismo tama침o
    min_len = min(len(data1), len(data2))
    data1 = data1[:min_len]
    data2 = data2[:min_len]
    
    signals.append(data1)
    signals.append(data2)
    
    # Apila las se침ales para mezclarlas
    mixed_signals = np.vstack(signals)
    return mixed_signals.T, sr  # Transpone la matriz para tener las se침ales en columnas
```
- Graficas mezcla y espectro mezcla mama:
- 
![Agregar](mezclamama.png)

![Agregar](espectromezclamama.png)

- Grficas mezcla y espectro mezcla mari:

![Agregar](mezclamari.png)

![Agregar](espectromezclamari.png)

<a name="ica"></a> 
## Uso de ICA, separacion y SNR relacion se침al-ruido

La funci칩n `aplicar_ica` est치 dise침ada para aplicar el An치lisis de Componentes Independientes (ICA) a una se침al de audio con el objetivo de separar las se침ales mezcladas en componentes individuales. Primero, se configura el ICA mediante la creaci칩n de una instancia de `FastICA` con dos componentes (`n_components=2`), que indica que el algoritmo intentar치 separar la se침al en dos componentes independientes. Adem치s, se establecen par치metros como `max_iter=1000` para el n칰mero m치ximo de iteraciones y `tol=0.001` para la tolerancia de convergencia del algoritmo. Luego, se aplica ICA a la se침al usando `ica.fit_transform(signal)`, que devuelve una matriz con las se침ales separadas. La funci칩n finalmente retorna esta matriz, donde cada columna representa una de las se침ales independientes obtenidas.

La funci칩n `potenciaDeSe침al` calcula la potencia de dos se침ales, una de voz y una de ruido. La potencia de una se침al se define como el promedio de los cuadrados de sus valores. Para calcular la potencia de la voz, se utiliza `np.mean(voz**2)`, y para la potencia del ruido, se usa `np.mean(ruido**2)`. Estos c치lculos permiten medir la intensidad de cada se침al en t칠rminos de su energ칤a promedio. La funci칩n retorna dos valores: la potencia de la voz y la potencia del ruido, facilitando la comparaci칩n entre ambos.

La funci칩n `identificar_voz` ayuda a determinar cu치l de las componentes separadas por ICA corresponde a la se침al de voz original. Primero, la funci칩n itera sobre las componentes obtenidas de ICA, graficando cada componente usando las funciones `graficarSonido` y `analisisEspectral` para mostrar la forma de onda y el espectro de frecuencias. Esto proporciona una visualizaci칩n que ayuda al usuario a identificar cu치l componente parece ser la voz. Despu칠s de mostrar las gr치ficas, el usuario debe seleccionar la componente que parece ser la voz a trav칠s de una entrada de texto. La funci칩n verifica si la selecci칩n es v치lida y devuelve la componente seleccionada. Si la selecci칩n no es v치lida, muestra un mensaje de error y retorna `None`. Este proceso permite identificar y extraer la se침al de voz de las componentes separadas.

```c
# Funci칩n para aplicar ICA y separar las se침ales
def aplicar_ica(signal):
    ica = FastICA(n_components=2, max_iter=1000, tol=0.001)  # Configuraci칩n del ICA
    se침ales_separadas = ica.fit_transform(signal)  # Aplica ICA para separar las se침ales
    
    return se침ales_separadas

# Funci칩n para calcular la potencia de la se침al
def potenciaDeSe침al(voz, ruido):
    potencia_voz = np.mean(voz**2)  # Calcula la potencia de la voz
    potencia_ruido = np.mean(ruido**2)  # Calcula la potencia del ruido
    return potencia_voz, potencia_ruido

# Funci칩n para identificar la componente de voz tras aplicar ICA
def identificar_voz(components, sr):
    # Asumimos que la componente m치s cercana a la forma de onda de la voz original es la voz
    for i, component in enumerate(components.T):
        graficarSonido(component, sr, f"Componente {i+1} para identificaci칩n de voz")
        analisisEspectral(component, sr, f"Espectro de la componente {i+1}")
    
    seleccion = int(input("\nSeleccione el n칰mero de la componente que parece ser la voz (1 o 2): ")) - 1
    if seleccion < 0 or seleccion >= len(components.T):
        print("Opci칩n no v치lida.")
        return None
    return components[:, seleccion]  # Devuelve la componente seleccionada
```

- Graficas voz separada y espectro voz separada mama:

![Agregar](vozseparadamama.png)

![Agregar](espectroseparadamama.png)

-SNR para la voz separada: 44.62 dB, es un valor bastante alto que indica una excelente calidad de la se침al de audio. En t칠rminos simples, el SNR mide la relaci칩n entre la intensidad de la se침al deseada (en este caso, la voz) y el nivel de ruido de fondo. Un SNR alto, sugiere que la se침al de voz es mucho m치s fuerte que el ruido, lo que significa que el ruido tiene una presencia m칤nima en la se침al final.

Este nivel de SNR implica que la voz separada es clara y bien distinguible, con poco impacto del ruido. Esto es crucial en aplicaciones de procesamiento de audio y comunicaci칩n, donde es importante mantener la claridad de la voz. En general, los valores de SNR m치s altos corresponden a una mejor calidad de la se침al, con menos interferencia del ruido.

- Graficas voz separada y espectro voz separada mari:

![Agregar](vozseparadamari.png)

![Agregar](espectroseparadamari.png)

-SNR para la voz separada: 45.37 dB, es a칰n mejor que el valor anterior de 44.62 dB, y sigue indicando una calidad de se침al excepcionalmente alta. Este valor sugiere que la se침al de voz es extremadamente clara en comparaci칩n con el nivel de ruido de fondo.

Con un SNR de 45.37 dB, la voz separada es muy n칤tida, con el ruido presente en la se침al siendo casi imperceptible. En t칠rminos pr치cticos, esto significa que la calidad de la se침al de voz es excelente, y el ruido tiene un impacto insignificante, si es que se percibe en absoluto. Este alto SNR es indicativo de una separaci칩n de se침ales muy efectiva y de un procesamiento de audio de alta calidad.

<a name="snr"></a> 
## Calculo SNR voces separadas

La funci칩n `opcion_calcular_snr` tiene como objetivo calcular la relaci칩n se침al-ruido (SNR) para cada par de archivos de audio de voz y ruido, y luego imprimir los resultados correspondientes. 

Primero, la funci칩n comienza con un bucle `for` que itera tres veces, una para cada par de archivos de audio especificado. El 칤ndice `i` se utiliza para acceder a los archivos en las listas `audio_voces` y `audio_ruido`, permitiendo procesar cada par de archivos en la iteraci칩n actual.

Dentro del bucle, la funci칩n carga los datos de audio para la voz y el ruido correspondientes al 칤ndice actual utilizando la funci칩n `loadAudio`. Esta funci칩n devuelve los datos de audio (`voz` y `ruido`) y la tasa de muestreo (`sr`). La tasa de muestreo se mantiene pero no se utiliza en c치lculos posteriores dentro de esta funci칩n.

Luego, se calcula la potencia de cada se침al utilizando la funci칩n `potenciaDeSe침al`, que toma como entrada los datos de `voz` y `ruido`. Esta funci칩n devuelve dos valores: `potencia_voz` y `potencia_ruido`, que representan la potencia de las se침ales de voz y ruido, respectivamente. La potencia se determina como el promedio del cuadrado de los valores de la se침al.

Con las potencias calculadas, se procede a calcular el SNR en decibelios usando la f칩rmula:

![Agregar](potencia.png)


La funci칩n `np.log10` calcula el logaritmo en base 10, y el resultado se multiplica por 10 para expresar el SNR en decibelios.


```c
# Funci칩n para calcular la relaci칩n se침al-ruido (SNR) para cada archivo
def opcion_calcular_snr():
    for i in range(3):
        voz, sr = loadAudio(audio_voces[i])
        ruido, sr = loadAudio(audio_ruido[i])
        potencia_voz, potencia_ruido = potenciaDeSe침al(voz, ruido)
        snr = 10.0 * np.log10(potencia_voz / potencia_ruido)  # Calcula el SNR en decibelios
        print(f"SNR para {audio_voces[i]}: {snr:.2f} dB")
```

<a name="menu"></a> 
## Menu

```c

```

<a name="analisis"></a> 
## Analisis

```c

```

<a name="contacto"></a> 
## Contacto
* Creado por [Marianitex](https://github.com/Marianitex) - s칤gueme en mis redes sociales como @_mariana.higuera











